{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca99812c-a75a-4e1f-8f30-c1a56b256601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "from regex_tokenizer import RegexTokenizer\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80e951e-663a-4cee-b4a9-c79275b9f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812e5ee5-32ba-409c-8cfb-6464e7dbd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77e9300-cc67-4157-be10-1f68be593979",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "directory = \"books\"\n",
    "\n",
    "for fn in os.listdir(directory):\n",
    "    with open(f\"{directory}/{fn}\", \"r\") as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "text = \"<|endoftext|>\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08796cd-a8dc-4d6d-a4db-c321d5b2ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ab23b7-ef5c-4a93-a1f2-e45d2c295766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 256\n",
    "\n",
    "assert vocab_size >= 256\n",
    "n_merges = vocab_size - 256\n",
    "\n",
    "tokenizer.train(text, n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c83632a-f203-4655-acbd-4b476f3dd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"tokenizer_256.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87aef11-71f2-4c73-bf9e-b0ea8ee5415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_1024.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57108767-ddde-4f55-a59c-1abf498aea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = tokenizer.encode(text, allow_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c551a90-dbec-42c0-8e0a-623aefc513f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"train_tokens_512.pkl\", \"rb\") as f:\n",
    "#    tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f9b474-e190-4ce5-a19d-2b05ae20619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_tokens_256.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496bdc5e-8c88-4f83-b370-0f47809abd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23967.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hi, I am Damian and this is ðŸ”¥\"\n",
    "sample_tokens = tokenizer.encode(sample_text, allow_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2dab0a-0c24-4f2b-80b5-878c44492677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 105,\n",
       " 44,\n",
       " 32,\n",
       " 73,\n",
       " 32,\n",
       " 97,\n",
       " 109,\n",
       " 32,\n",
       " 68,\n",
       " 97,\n",
       " 109,\n",
       " 105,\n",
       " 97,\n",
       " 110,\n",
       " 32,\n",
       " 97,\n",
       " 110,\n",
       " 100,\n",
       " 32,\n",
       " 116,\n",
       " 104,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 105,\n",
       " 115,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 148,\n",
       " 165]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488e6c9b-3536-4dbb-9c87-c3f8767578f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Damian and this is ðŸ”¥\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be3ffed-bdf6-4b33-8f08-76d7d7daa26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, context_size):\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx:(idx + self.context_size)]\n",
    "        y = self.tokens[(idx + 1):(idx + self.context_size + 1)]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d829f00-7f36-4ba8-a160-9b2f29d4697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "694fbd63-224c-4dea-9e01-452626bc2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = TokenDataset(tokens, context_size = context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d9f11f-49d5-4186-b14f-6d7b930a5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(token_dataset, [0.9, 0.05, 0.05])\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f44c4f3-e687-4e05-9cf9-7b6a5471624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cef69763-3202-4be3-bd3a-8234ef641313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, train_loader, val_loader, n_epochs,\n",
    "          optimizer=None,\n",
    "          lr_scheduler=None,\n",
    "          early_stopper=None,\n",
    "          metrics_per_epoch=10\n",
    "         ):\n",
    "    \n",
    "    transformer = transformer.to(device)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(transformer.parameters(), lr=3e-4)\n",
    "        print(\"Using default optimizer\")\n",
    "        \n",
    "    if early_stopper is None:\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-2)\n",
    "        print(\"Using default early stopper\")\n",
    "        \n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                  factor=0.3, patience=3, min_lr=1e-5,\n",
    "                                                                  threshold=1e-3\n",
    "                                                                 )\n",
    "        print(\"Using default LR scheduler\")\n",
    "    \n",
    "    # label smoothing deactivated for now\n",
    "    criterion_train = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses_over_epochs = []\n",
    "    val_losses_over_epochs = []\n",
    "    \n",
    "    metrics_every = len(train_loader) // metrics_per_epoch\n",
    "    in_between_epochs = []\n",
    "    in_between_metrics = []\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        train_losses_this_batch = []\n",
    "        transformer.train()\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch_idx + 1}/{n_epochs}\", unit=\"batch\") as tepoch:\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(tepoch):\n",
    "\n",
    "                # to GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                logits = transformer(batch_x)\n",
    "\n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion_train(logits, batch_y)\n",
    "\n",
    "                train_losses_this_batch.append(loss.item())\n",
    "\n",
    "                if (batch_idx + 1) % metrics_every == 0:\n",
    "                    in_between_loss = np.mean(np.array(train_losses_this_batch[-metrics_every:]))\n",
    "                    in_between_metrics.append(in_between_loss)\n",
    "                    in_between_epochs.append(epoch_idx + (batch_idx / len(train_loader)))\n",
    "\n",
    "                    tepoch.set_postfix(avg_loss=in_between_loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        train_loss_this_epoch = np.mean(np.array(train_losses_this_batch))\n",
    "        train_losses_over_epochs.append(train_loss_this_epoch)\n",
    "        \n",
    "        # for early stopping\n",
    "        val_losses_this_batch = []\n",
    "        \n",
    "        transformer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(val_loader):\n",
    "\n",
    "                # to GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                logits = transformer(batch_x)\n",
    "                \n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion_test(logits, batch_y)\n",
    "\n",
    "                val_losses_this_batch.append(loss.item())\n",
    "        \n",
    "        val_loss_this_epoch = np.mean(np.array(val_losses_this_batch))\n",
    "        val_losses_over_epochs.append(val_loss_this_epoch)\n",
    "        print(f\"{epoch_idx}. avg. train loss = {train_loss_this_epoch}, avg. val loss = {val_loss_this_epoch}\")\n",
    "        \n",
    "        should_stop = early_stopper.early_stop(val_loss_this_epoch)\n",
    "        lr_scheduler.step(val_loss_this_epoch)\n",
    "        \n",
    "        if should_stop:\n",
    "            print(f\"stopping early (val. loss did not decrease for {early_stopper.patience})\")\n",
    "            break\n",
    "        \n",
    "    return train_losses_over_epochs, in_between_epochs, in_between_metrics, val_losses_over_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e73e55d-45e3-474b-bc87-c136b464b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256\n",
    "n_symbols = vocab_size + len(tokenizer.special_tokens)\n",
    "\n",
    "transformer = Transformer(n_symbols, context_size, d_model = 256, n_heads = 8, n_layers=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "175b231b-0e8e-463c-8599-a4957a248e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 5939969\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Get the number of parameters\n",
    "num_params = count_parameters(transformer)\n",
    "print(f\"Number of trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68b4d2b-8123-40e6-b30a-f0bbb52d209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_1024_18e.pth\n"
     ]
    }
   ],
   "source": [
    "transformer.load_model(\"model_1024_18e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb35476e-85cc-4084-bb64-7fac054129d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    transformer = torch.nn.DataParallel(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d660546-ce9d-4fc7-b927-751759173f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer = transformer.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c722879-7383-4c17-ab73-2ce44029933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff6d1628-9e1b-40eb-8b89-3df59796b869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default early stopper\n",
      "Using default LR scheduler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:42<00:00,  9.18batch/s, avg_loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. avg. train loss = 1.2537779447339532, avg. val loss = 1.044516016377343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:40<00:00,  9.20batch/s, avg_loss=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. avg. train loss = 1.0518828357783927, avg. val loss = 0.9038003379768795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:39<00:00,  9.21batch/s, avg_loss=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. avg. train loss = 0.9674982047823015, avg. val loss = 0.8126448636584812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:40<00:00,  9.20batch/s, avg_loss=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. avg. train loss = 0.9132949791676589, avg. val loss = 0.7446919899516635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:40<00:00,  9.19batch/s, avg_loss=0.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. avg. train loss = 0.874725679880427, avg. val loss = 0.6991754105356005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:39<00:00,  9.21batch/s, avg_loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. avg. train loss = 0.845420897426584, avg. val loss = 0.6621509642071194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:38<00:00,  9.22batch/s, avg_loss=0.814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. avg. train loss = 0.822179857449604, avg. val loss = 0.631395171350903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:37<00:00,  9.23batch/s, avg_loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. avg. train loss = 0.8031615089218868, avg. val loss = 0.6052261923419104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8097/8097 [14:37<00:00,  9.23batch/s, avg_loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. avg. train loss = 0.7871972238250436, avg. val loss = 0.5903909688525729\n"
     ]
    }
   ],
   "source": [
    "train_losses, in_between_epochs, in_between_metrics, val_losses =\\\n",
    "train(transformer, train_loader, val_loader, n_epochs=9, optimizer=optimizer, metrics_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd8d723-c335-4835-9ca7-d52b329b2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.module.save_model(\"model_256_9e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b3eb2f-186e-4075-b6f4-0c83a41f4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: [65, 727, 811, 292, 279, 564, 511, 568, 263, 306, 531, 121, 355, 429, 262, 359, 987, 856, 260, 308, 425, 280, 46, 387, 609, 1006, 331, 44, 429, 347, 479, 98, 650, 586, 258, 379, 319, 269, 666, 798, 932, 907, 817, 339, 829, 671, 258, 550, 314, 271, 401, 104, 46, 544, 967, 771, 274, 475, 348, 117, 538, 46]\n"
     ]
    }
   ],
   "source": [
    "if isinstance(transformer, nn.DataParallel):\n",
    "    print(\"removed wrapper\")\n",
    "    transformer = transformer.module\n",
    "\n",
    "\n",
    "prompt = \"While exploring the depths of the ocean in the Nautilus, Captain Nemo discovers an underwater cave filled with artifacts. Among these, he finds a detailed map that leads to a hidden location on land.\"\n",
    "n_tokens = 128\n",
    "n_samples = 3\n",
    "\n",
    "responses = transformer.sample(prompt_tokens, n_tokens, n_samples, beta = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96054cde-255a-4aef-b800-e263216acdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reponses(prompt, responses):\n",
    "    print(\"Prompt:\")\n",
    "    print(\"```\")\n",
    "    print(f\"{prompt}\")\n",
    "    print(\"```\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"vocab size: 1024\")\n",
    "    print(\"\")\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "\n",
    "        response_tokens = [t.item() for t in response.cpu().detach()]\n",
    "        response_text = tokenizer.decode(response_tokens)\n",
    "        print(\"```\")\n",
    "        print(f\"{prompt} {response_text}\")\n",
    "        print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d1eda80-7809-4841-9b5f-d4573b27cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "```\n",
      "While exploring the depths of the ocean in the Nautilus, Captain Nemo discovers an underwater cave filled with artifacts. Among these, he finds a detailed map that leads to a hidden location on land.\n",
      "```\n",
      "\n",
      "vocab size: 1024\n",
      "\n",
      "```\n",
      "While exploring the depths of the ocean in the Nautilus, Captain Nemo discovers an underwater cave filled with artifacts. Among these, he finds a detailed map that leads to a hidden location on land. \n",
      "now, when there was one of the scent occurred which I was keeping on earth\n",
      "and open, and I could see nothing but a strange dress which I took my bank\n",
      "to easy open to me, and I found my smack ones and stopped at the dog.\n",
      "There was a perfect rifle of cards, around the parlor, and out-of-wheel\n",
      "whispered, and big old snakes, and spiders, and a\n",
      "```\n",
      "```\n",
      "While exploring the depths of the ocean in the Nautilus, Captain Nemo discovers an underwater cave filled with artifacts. Among these, he finds a detailed map that leads to a hidden location on land. \n",
      "Captain Nemo, dancing hotels of men on the pillows. He looked\n",
      "slow, and was an old scrape, and skilk upon the floor door of the eye\n",
      "two rose at and out for a match. I passed through the danger which was\n",
      "burnt. The place it began to hold the branches of the large snories, but\n",
      "concluded indicated anger at the end of the detail\n",
      "```\n",
      "```\n",
      "While exploring the depths of the ocean in the Nautilus, Captain Nemo discovers an underwater cave filled with artifacts. Among these, he finds a detailed map that leads to a hidden location on land. \n",
      "Certainly, in the midst of a womanâ€™s apronicleâ€™s name. â€œI do not fish for\n",
      "me, for I have one or two pounds.â€\n",
      "\n",
      "â€œBut, sir, I think that the world has been finished.â€\n",
      "\n",
      "â€œAnd now?â€ I asked, contracting it fully with a frightened eyes. â€œIn that\n",
      "isâ€”â€\n",
      "\n",
      "â€œVery well, sir.â€\n",
      "\n",
      "â€œBut what do you say, Professor?â€\n",
      "\n",
      "â€œWhy not?â€\n",
      "\n",
      "â€œYou must not be solved.â€\n",
      "\n",
      "â€œHow\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print_reponses(prompt, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37f7ec-4636-41ed-b47c-75aa66b7307e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac7582-060d-4a32-98a7-fcd53ac1340d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
